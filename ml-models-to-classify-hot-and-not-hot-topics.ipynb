{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport spacy\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-17T22:41:59.338164Z","iopub.execute_input":"2022-03-17T22:41:59.338609Z","iopub.status.idle":"2022-03-17T22:41:59.419216Z","shell.execute_reply.started":"2022-03-17T22:41:59.338558Z","shell.execute_reply":"2022-03-17T22:41:59.418295Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Load the data\nrecords = pd.read_csv('../input/scirate-quant-ph/scirate_quant-ph.csv', dtype={\"id\": str},\n                        index_col = 0)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-17T22:36:26.335588Z","iopub.execute_input":"2022-03-17T22:36:26.335977Z","iopub.status.idle":"2022-03-17T22:36:26.421700Z","shell.execute_reply.started":"2022-03-17T22:36:26.335943Z","shell.execute_reply":"2022-03-17T22:36:26.420994Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# see the first rows of data\nrecords.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T22:38:20.727511Z","iopub.execute_input":"2022-03-17T22:38:20.727793Z","iopub.status.idle":"2022-03-17T22:38:20.741219Z","shell.execute_reply.started":"2022-03-17T22:38:20.727765Z","shell.execute_reply":"2022-03-17T22:38:20.740350Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# check for missing values in each column\nrecords.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T22:39:09.677794Z","iopub.execute_input":"2022-03-17T22:39:09.678717Z","iopub.status.idle":"2022-03-17T22:39:09.688174Z","shell.execute_reply.started":"2022-03-17T22:39:09.678680Z","shell.execute_reply":"2022-03-17T22:39:09.687140Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# the dimension of data\nrecords.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-17T22:39:29.104588Z","iopub.execute_input":"2022-03-17T22:39:29.105018Z","iopub.status.idle":"2022-03-17T22:39:29.109505Z","shell.execute_reply.started":"2022-03-17T22:39:29.104988Z","shell.execute_reply":"2022-03-17T22:39:29.108849Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# 1. EDA","metadata":{}},{"cell_type":"code","source":"records['date_parsed'] = pd.to_datetime(records[[\"year\", \"month\", \"day\"]], format=\"%Y/%m/%d\")\nrecords[\"dayofweek\"] = records['date_parsed'].dt.day_name()\n\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, sharey=True, figsize=(15, 6))\nsns.stripplot(x=\"day\", y=\"scites\", data=records, jitter=True, ax=ax1)\nsns.stripplot(x=\"month\", y=\"scites\", data=records, jitter=True, ax=ax2)\nsns.stripplot(x=\"year\", y=\"scites\", data=records, jitter=True, ax=ax3)\ndays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\nsns.stripplot(x=\"dayofweek\", y=\"scites\", data=records, order=days, jitter=True, ax=ax4)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T22:42:03.414547Z","iopub.execute_input":"2022-03-17T22:42:03.414865Z","iopub.status.idle":"2022-03-17T22:42:04.689411Z","shell.execute_reply.started":"2022-03-17T22:42:03.414831Z","shell.execute_reply":"2022-03-17T22:42:04.688559Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# 2. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"**2.1 Vectorization (word embedding) of text (abstract, title) features**","metadata":{}},{"cell_type":"markdown","source":"The first step of featute engineering: I need to represent the text in the data numerically. Instead of using a bag of words representations for texts (abstracts, titles, authors), I will use word embeddings (or word vectors). SpaCy provides embeddings accessible by a large language model (en_core_web_lg).","metadata":{}},{"cell_type":"code","source":"# Load the large model to get the vectors\nnlp = spacy.load('en_core_web_lg')\n","metadata":{"execution":{"iopub.status.busy":"2022-03-17T22:46:44.751042Z","iopub.execute_input":"2022-03-17T22:46:44.752880Z","iopub.status.idle":"2022-03-17T22:46:50.402375Z","shell.execute_reply.started":"2022-03-17T22:46:44.752818Z","shell.execute_reply":"2022-03-17T22:46:50.401482Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Here, I will load the abstract, title, authors and convert each of them to document vectors (of 300-dimensional).","metadata":{}},{"cell_type":"code","source":"# get the vectors for abstracts\nwith nlp.disable_pipes():\n    vectors = np.array([nlp(abstract).vector for abstract in records.abstract])\n    \nvectors.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-17T22:47:17.634133Z","iopub.execute_input":"2022-03-17T22:47:17.634443Z","iopub.status.idle":"2022-03-17T22:48:24.294838Z","shell.execute_reply.started":"2022-03-17T22:47:17.634409Z","shell.execute_reply":"2022-03-17T22:48:24.294234Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# get the vectors for titles\nwith nlp.disable_pipes():\n    vectors_title = np.array([nlp(title).vector for title in records.title])\n    \nvectors_title.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-17T22:53:28.992560Z","iopub.execute_input":"2022-03-17T22:53:28.993506Z","iopub.status.idle":"2022-03-17T22:53:42.396587Z","shell.execute_reply.started":"2022-03-17T22:53:28.993447Z","shell.execute_reply":"2022-03-17T22:53:42.395495Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# get the vectors for authors\nwith nlp.disable_pipes():\n    vectors_authors = np.array([nlp(authors).vector for authors in records.authors])\n    \nvectors_authors.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-17T22:53:58.087552Z","iopub.execute_input":"2022-03-17T22:53:58.088328Z","iopub.status.idle":"2022-03-17T22:54:10.770246Z","shell.execute_reply.started":"2022-03-17T22:53:58.088288Z","shell.execute_reply":"2022-03-17T22:54:10.769349Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"The second step of featute engineering: I will make a new column (records.dayofweek) indicating the day-of-week for each paper (or for each date). ","metadata":{}},{"cell_type":"markdown","source":"**2.2 Submission_date_of_paper features**","metadata":{}},{"cell_type":"code","source":"records['date_parsed'] = pd.to_datetime(records[[\"year\", \"month\", \"day\"]], format=\"%Y/%m/%d\")\nrecords[\"dayofweek\"] = records['date_parsed'].dt.day_name()\nrecords.dayofweek = records.dayofweek.map({\"Monday\":1, \"Tuesday\":2, \"Wednesday\":3, \"Thursday\":4, \"Friday\":5, \"Saturday\":6, \"Sunday\":7})","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:09:58.044008Z","iopub.execute_input":"2022-03-17T23:09:58.044436Z","iopub.status.idle":"2022-03-17T23:09:58.063515Z","shell.execute_reply.started":"2022-03-17T23:09:58.044404Z","shell.execute_reply":"2022-03-17T23:09:58.062703Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"**2.3 Daily_order_of_paper features**","metadata":{}},{"cell_type":"markdown","source":"The third step of feature engineering: I will make a new dataframe (daily_order) including the order of the paper per date and the total papers per date (daily_order[[\"total\", \"order\"]]), and will concatenate it to the original dataset. \n","metadata":{}},{"cell_type":"code","source":"daily_order = []\nfor date in pd.to_datetime(pd.DataFrame(records['date_parsed'].unique(), columns=[\"date\"], dtype='object')[\"date\"]):\n    same_day_df = records[(records[\"year\"] == date.year) &\n                       (records[\"month\"] == date.month) &\n                       (records[\"day\"] == date.day)].sort_values(\"id\")\n    total = len(same_day_df)\n    daily_order.extend([[total, order, scites, (same_day_df[\"id\"]).iloc[order]] for order, scites in enumerate(same_day_df[\"scites\"])])\ndaily_order = pd.DataFrame(daily_order, columns=[\"total\", \"order\", \"scites_2\", \"id\"])\n\nrecords = pd.concat([records, daily_order[[\"total\", \"order\"]]], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:10:01.892382Z","iopub.execute_input":"2022-03-17T23:10:01.892650Z","iopub.status.idle":"2022-03-17T23:10:03.289888Z","shell.execute_reply.started":"2022-03-17T23:10:01.892621Z","shell.execute_reply":"2022-03-17T23:10:03.289091Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(daily_order, col=\"total\", col_wrap=3, hue=\"order\")\ng.map(sns.scatterplot, \"order\", \"scites_2\")\n\nfig, ax5 = plt.subplots(1, figsize=(15, 6))\nsns.stripplot(x=\"order\", y=\"scites_2\", data=daily_order, jitter=True, ax=ax5)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T22:51:45.746667Z","iopub.execute_input":"2022-03-17T22:51:45.746965Z","iopub.status.idle":"2022-03-17T22:51:48.945991Z","shell.execute_reply.started":"2022-03-17T22:51:45.746933Z","shell.execute_reply":"2022-03-17T22:51:48.945122Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"**2.4 Is any frequent author in the list of authors**","metadata":{}},{"cell_type":"markdown","source":"The fourth step of feature engineering: I will find the frequency of each single author in the document, and then create a series of the most frequent authors (frequent_authors) by only keeping the authors with the frequency more than 15.  \nNext, by defining a function (any_frequent_authors), I will check if the list of authors for each paper includes any of the frequent authors, and finally will add a binary column to the records data (records['any_frequent_authors']) based on this analysis. ","metadata":{}},{"cell_type":"code","source":"frequent_authors = pd.Series((\";\".join(records[\"authors\"])).split(\";\")).value_counts().iloc[:37]\n\ndef any_frequent_authors(authors, frequent_authors):\n    for frequent_author in frequent_authors.index:\n        if frequent_author in authors:\n            return 1\n    return 0\n\n\nrecords['any_frequent_authors'] = records.authors.str.split(\";\").apply(lambda x: any_frequent_authors(x, frequent_authors))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:10:09.604791Z","iopub.execute_input":"2022-03-17T23:10:09.605073Z","iopub.status.idle":"2022-03-17T23:10:09.643865Z","shell.execute_reply.started":"2022-03-17T23:10:09.605045Z","shell.execute_reply":"2022-03-17T23:10:09.643224Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"Now I will concatenate all the new engineered features to the existing numerical features to create X. ","metadata":{}},{"cell_type":"code","source":"X_ = records[['year', 'month', 'day', 'dayofweek', 'total', 'order', 'any_frequent_authors']].values\n\n\n# concatenate all the numerical features to create X\nX = np.concatenate([vectors, vectors_title, vectors_authors, X_], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:41:50.909257Z","iopub.execute_input":"2022-03-17T23:41:50.909699Z","iopub.status.idle":"2022-03-17T23:41:50.919266Z","shell.execute_reply.started":"2022-03-17T23:41:50.909655Z","shell.execute_reply":"2022-03-17T23:41:50.918460Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":"Here, I want to define my project as a classification task between hot or not-hot topics in quantum physics based on the number of paper's citation. \nI will assume papers with scites>19 as hot topic (with label 1) and with scites<=19 as not-hot topic (with label 0).\nNote 19 is the mean value for citations. (int(records.scites.mean())=19).\nbased on this I will create the target series. ","metadata":{}},{"cell_type":"code","source":"# Create the target series\ny = (records.scites > 19).astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:41:55.402472Z","iopub.execute_input":"2022-03-17T23:41:55.403186Z","iopub.status.idle":"2022-03-17T23:41:55.408605Z","shell.execute_reply.started":"2022-03-17T23:41:55.403121Z","shell.execute_reply":"2022-03-17T23:41:55.407694Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"g = sns.countplot(y)\ng.set_xticklabels([\"not-hot\", \"hot\"])\ng.set_xlabel(\"topic\")","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:39:20.661010Z","iopub.execute_input":"2022-03-17T23:39:20.661315Z","iopub.status.idle":"2022-03-17T23:39:20.840346Z","shell.execute_reply.started":"2022-03-17T23:39:20.661281Z","shell.execute_reply":"2022-03-17T23:39:20.839484Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"(y.value_counts()).plot(kind = \"pie\", autopct='%1.1f')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:40:21.391659Z","iopub.execute_input":"2022-03-17T23:40:21.392318Z","iopub.status.idle":"2022-03-17T23:40:21.493666Z","shell.execute_reply.started":"2022-03-17T23:40:21.392280Z","shell.execute_reply":"2022-03-17T23:40:21.493153Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"65% of papers are in hot-topic class, and 35% are not hot topic. The classes are not balanced. ","metadata":{}},{"cell_type":"markdown","source":"I will first split the data (X, y) to train-data and test-data, and then train different classification ML models.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25, stratify=y, random_state=1)\nfrom sklearn.metrics import recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:42:16.054288Z","iopub.execute_input":"2022-03-17T23:42:16.054826Z","iopub.status.idle":"2022-03-17T23:42:16.066396Z","shell.execute_reply.started":"2022-03-17T23:42:16.054778Z","shell.execute_reply":"2022-03-17T23:42:16.065629Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"markdown","source":"# 3. Machine Learning Models","metadata":{}},{"cell_type":"code","source":"def ML_models_performance(model, X_train, y_train ,X_test ,y_test, y_pred, model_name):\n \n    performance_df=pd.DataFrame({'Train_accuracy':model.score(X_train,y_train),\"Test_accuracy\":model.score(X_test,y_test),\n                       \"Precision\":precision_score(y_pred,y_test),\"Recall\":recall_score(y_pred,y_test),\n                       \"F1_Score\":f1_score(y_pred,y_test)}, index=[model_name])\n    return performance_df","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:42:19.297870Z","iopub.execute_input":"2022-03-17T23:42:19.298326Z","iopub.status.idle":"2022-03-17T23:42:19.303137Z","shell.execute_reply.started":"2022-03-17T23:42:19.298265Z","shell.execute_reply":"2022-03-17T23:42:19.302586Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n# Create the LogisticRegression model\nlr = LogisticRegression(solver='sag')\n# Fit the model\nlr.fit(X_train, y_train)\n# Use the trained model to predict\ny_pred = lr.predict(X_test)\n# model accuracy\nprint(f'Model train accuracy: {lr.score(X_train, y_train)*100:.3f}%')\nprint(f'Model test accuracy: {lr.score(X_test, y_test)*100:.3f}%')\nprint(f'Model test precision: {precision_score(y_pred,y_test):.3f}')\nprint(f'Model test recall: {recall_score(y_pred,y_test):.3f}')\nprint(f'Model test f1_score: {f1_score(y_pred,y_test):.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:42:26.551537Z","iopub.execute_input":"2022-03-17T23:42:26.551844Z","iopub.status.idle":"2022-03-17T23:42:27.566430Z","shell.execute_reply.started":"2022-03-17T23:42:26.551785Z","shell.execute_reply":"2022-03-17T23:42:27.565571Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"lr_performance = ML_models_performance(lr, X_train, y_train ,X_test ,y_test, y_pred, \"Logisitc Regression\")\nlr_performance","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:26:18.876176Z","iopub.execute_input":"2022-03-17T23:26:18.876440Z","iopub.status.idle":"2022-03-17T23:26:18.903263Z","shell.execute_reply.started":"2022-03-17T23:26:18.876413Z","shell.execute_reply":"2022-03-17T23:26:18.902475Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\n# Create the LinearSVC model with some regularization\nLSVC = LinearSVC(random_state=1, dual=False, C=1/2)\n# Fit the model\nLSVC.fit(X_train, y_train)\n# Use the trained model to predict\ny_pred = LSVC.predict(X_test)\n# model accuracy\nprint(f'Model train accuracy: {LSVC.score(X_train, y_train)*100:.3f}%')\nprint(f'Model test accuracy: {LSVC.score(X_test, y_test)*100:.3f}%')\nprint(f'Model test precision: {precision_score(y_pred,y_test):.3f}')\nprint(f'Model test recall: {recall_score(y_pred,y_test):.3f}')\nprint(f'Model test f1_score: {f1_score(y_pred,y_test):.3f}')\n","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:42:42.344081Z","iopub.execute_input":"2022-03-17T23:42:42.344369Z","iopub.status.idle":"2022-03-17T23:42:43.188750Z","shell.execute_reply.started":"2022-03-17T23:42:42.344340Z","shell.execute_reply":"2022-03-17T23:42:43.187573Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"LSVC_performance = ML_models_performance(LSVC, X_train, y_train ,X_test ,y_test, y_pred, \"LinearSVC\")\nLSVC_performance","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:26:58.082767Z","iopub.execute_input":"2022-03-17T23:26:58.083056Z","iopub.status.idle":"2022-03-17T23:26:58.112768Z","shell.execute_reply.started":"2022-03-17T23:26:58.083026Z","shell.execute_reply":"2022-03-17T23:26:58.111976Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n# Create the RandomForest model\nrfc = RandomForestClassifier(random_state=0)\n# Fit the model\nrfc.fit(X_train, y_train)\n# Use the trained model to predict\ny_pred = rfc.predict(X_test)\n# model accuracy\nprint(f'Model train accuracy: {rfc.score(X_train, y_train)*100:.3f}%')\nprint(f'Model test accuracy: {rfc.score(X_test, y_test)*100:.3f}%')\nprint(f'Model test precision: {precision_score(y_pred,y_test):.3f}')\nprint(f'Model test recall: {recall_score(y_pred,y_test):.3f}')\nprint(f'Model test f1_score: {f1_score(y_pred,y_test):.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:27:58.672883Z","iopub.execute_input":"2022-03-17T23:27:58.673146Z","iopub.status.idle":"2022-03-17T23:28:01.932992Z","shell.execute_reply.started":"2022-03-17T23:27:58.673118Z","shell.execute_reply":"2022-03-17T23:28:01.932248Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"rfc_performance = ML_models_performance(rfc, X_train, y_train ,X_test ,y_test, y_pred, \"Random Forest\")\nrfc_performance","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:28:13.601712Z","iopub.execute_input":"2022-03-17T23:28:13.602577Z","iopub.status.idle":"2022-03-17T23:28:13.683350Z","shell.execute_reply.started":"2022-03-17T23:28:13.602519Z","shell.execute_reply":"2022-03-17T23:28:13.682823Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\n# Create the SVC model with some regularization\nsvcl = SVC(kernel='linear', random_state=1, C=1/2)\n# Fit the model\nsvcl.fit(X_train, y_train)\n# Use the trained model to predict\ny_pred = svcl.predict(X_test)\n# model accuracy\nprint(f'Model train accuracy: {svcl.score(X_train, y_train)*100:.3f}%')\nprint(f'Model test accuracy: {svcl.score(X_test, y_test)*100:.3f}%')\nprint(f'Model test precision: {precision_score(y_pred,y_test):.3f}')\nprint(f'Model test recall: {recall_score(y_pred,y_test):.3f}')\nprint(f'Model test f1_score: {f1_score(y_pred,y_test):.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:28:36.221063Z","iopub.execute_input":"2022-03-17T23:28:36.221498Z","iopub.status.idle":"2022-03-17T23:28:41.051780Z","shell.execute_reply.started":"2022-03-17T23:28:36.221466Z","shell.execute_reply":"2022-03-17T23:28:41.050789Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"comparison_df = pd.concat([lr_performance, LSVC_performance, rfc_performance])\ncomparison_df","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:29:05.720493Z","iopub.execute_input":"2022-03-17T23:29:05.720790Z","iopub.status.idle":"2022-03-17T23:29:05.734771Z","shell.execute_reply.started":"2022-03-17T23:29:05.720756Z","shell.execute_reply":"2022-03-17T23:29:05.733979Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":"# 4. Finding similar papers","metadata":{}},{"cell_type":"markdown","source":"One of the advantages of using word vectors is that we can find papers with similar content. Because papers (more importantly their abstracts) with similar content generally have similar vectors. Similar papers can be found by measuring the similarity between the vectors of their abstracts. A metric for this is the cosine similarity measuring the angle between two vectors, and it is the inner product of two vectors, divided by the magnitudes of each vector. The cosine similarity varies between -1 and 1, which corresponds to complete opposite and perfect similarity, respectively.","metadata":{}},{"cell_type":"code","source":"# A given abstract\nabstract = \"\"\"Free-space channels provide the possibility of establishing continuous-variable quantum key \ndistribution in global communication networks. However, the fluctuating nature of transmissivity in these \nchannels introduces an extra noise which reduces the achievable secret key rate. \nWe consider two classical postprocessing strategies, postselection of high-transmissivity data and data \nclusterization, to reduce the fluctuation-induced noise of the channel. We undertake the investigation of\nsuch strategies utilizing a composable security proof in a realistic finite-size regime against both\ncollective and individual attacks. We also present an efficient parameter estimation approach to\nestimate the effective Gaussian parameters over the postselected data or the clustered data.\nAlthough the composable finite-size effects become more significant with the postselection\nand clusterization both reducing the size of the data, our results show that these strategies are\nstill able to enhance the finite-size key rate against both individual and collective attacks with\na remarkable improvement against collective attacks, \neven moving the protocol from an insecure regime to a secure regime under certain conditions..\"\"\"\n\ndef cosine_similarity(a, b):\n    return np.dot(a, b)/np.sqrt(a.dot(a)*b.dot(b))\n\n# Get the vecor of the given abstract\nabstract_vec = nlp(abstract).vector\n\n# Calculate the mean for the abstract vectors, with shape (300,)\nvec_mean = vectors.mean(axis=0)\n# Subtract the mean from the vectors\ncentered = vectors - vec_mean\n\n# Calculate similarities between the given abstract and each abstract in the dataset\n# We also need to subtract the vec_mean from the abstract_vec\nsims = np.array([cosine_similarity(abstract_centered, abstract_vec - vec_mean) for abstract_centered in centered])\n\n# Get the index for the most similar abstract\nmost_similar = sims.argmax()\nprint(records.iloc[most_similar].abstract)\nprint(f\"cosine_similarity is {sims[most_similar]:.3f}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:12:52.415906Z","iopub.execute_input":"2022-03-17T23:12:52.416486Z","iopub.status.idle":"2022-03-17T23:12:52.490726Z","shell.execute_reply.started":"2022-03-17T23:12:52.416435Z","shell.execute_reply":"2022-03-17T23:12:52.490006Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"Now I will change the strategy for the first step of the featute engineering: I need to represent the text in the data numerically. Instead of using the word embeddings (or word vectors), I will use a bag of words representations for texts (abstracts, titles, authors). In fact, I will convert a collection of text features to a matrix of word counts using TfidfVectorizer.\n","metadata":{}},{"cell_type":"markdown","source":"# 5. Use a different vectorization technique to vectorize text features","metadata":{}},{"cell_type":"code","source":"# text features\nX1 = records.authors\nX2 = records.title\nX3 = records.abstract\n\n# convert a collection of text features to a matrix of word counts\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer()\nX1 = (tfidf.fit_transform(X1)).toarray()\nX2 = (tfidf.fit_transform(X2)).toarray()\nX3 = (tfidf.fit_transform(X3)).toarray()\n\n# concatenate all the numerical features to create new X\nX = np.concatenate([X1, X2, X3, X_], axis=1)\n# split the data (X, y) to train-data and test-data\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25, stratify=y, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:13:13.610387Z","iopub.execute_input":"2022-03-17T23:13:13.610678Z","iopub.status.idle":"2022-03-17T23:13:14.912929Z","shell.execute_reply.started":"2022-03-17T23:13:13.610650Z","shell.execute_reply":"2022-03-17T23:13:14.911926Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"I will now use the same classification models (used earlier) to train the new X. ","metadata":{}},{"cell_type":"code","source":"def ML_models_performance(model, X_train, y_train ,X_test ,y_test, y_pred, model_name):\n \n    performance_df=pd.DataFrame({'Train_accuracy':model.score(X_train,y_train),\"Test_accuracy\":model.score(X_test,y_test),\n                       \"Precision\":precision_score(y_pred,y_test),\"Recall\":recall_score(y_pred,y_test),\n                       \"F1_Score\":f1_score(y_pred,y_test)}, index=[model_name])\n    return performance_df","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:16:34.020103Z","iopub.execute_input":"2022-03-17T23:16:34.020426Z","iopub.status.idle":"2022-03-17T23:16:34.026549Z","shell.execute_reply.started":"2022-03-17T23:16:34.020392Z","shell.execute_reply":"2022-03-17T23:16:34.025730Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n# Create the LogisticRegression model\nlr = LogisticRegression(solver='sag')\n# Fit the model\nlr.fit(X_train, y_train)\n# Use the trained model to predict\ny_pred = lr.predict(X_test)\n# model accuracy\nprint(f'Model train accuracy: {lr.score(X_train, y_train)*100:.3f}%')\nprint(f'Model test accuracy: {lr.score(X_test, y_test)*100:.3f}%')\nprint(f'Model test precision: {precision_score(y_pred,y_test):.3f}')\nprint(f'Model test recall: {recall_score(y_pred,y_test):.3f}')\nprint(f'Model test f1_score: {f1_score(y_pred,y_test):.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:17:05.454075Z","iopub.execute_input":"2022-03-17T23:17:05.454383Z","iopub.status.idle":"2022-03-17T23:17:22.460286Z","shell.execute_reply.started":"2022-03-17T23:17:05.454351Z","shell.execute_reply":"2022-03-17T23:17:22.455106Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"lr_performance = ML_models_performance(lr, X_train, y_train ,X_test ,y_test, y_pred, \"Logisitc Regression\")\nlr_performance","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:17:26.819686Z","iopub.execute_input":"2022-03-17T23:17:26.820011Z","iopub.status.idle":"2022-03-17T23:17:26.883180Z","shell.execute_reply.started":"2022-03-17T23:17:26.819979Z","shell.execute_reply":"2022-03-17T23:17:26.882363Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# Create the LinearSVC model with some regularization\nLSVC = LinearSVC(random_state=1, dual=False, C=1/1.2)\n# Fit the model\nLSVC.fit(X_train, y_train)\n# Use the trained model to predict\ny_pred = LSVC.predict(X_test)\n# model accuracy\nprint(f'Model train accuracy: {LSVC.score(X_train, y_train)*100:.3f}%')\nprint(f'Model test accuracy: {LSVC.score(X_test, y_test)*100:.3f}%')\nprint(f'Model test precision: {precision_score(y_pred,y_test):.3f}')\nprint(f'Model test recall: {recall_score(y_pred,y_test):.3f}')\nprint(f'Model test f1_score: {f1_score(y_pred,y_test):.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:18:06.952261Z","iopub.execute_input":"2022-03-17T23:18:06.952553Z","iopub.status.idle":"2022-03-17T23:18:07.285189Z","shell.execute_reply.started":"2022-03-17T23:18:06.952525Z","shell.execute_reply":"2022-03-17T23:18:07.284210Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"LSVC_performance = ML_models_performance(LSVC, X_train, y_train ,X_test ,y_test, y_pred, \"LinearSVC\")\nLSVC_performance","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:18:25.648626Z","iopub.execute_input":"2022-03-17T23:18:25.649345Z","iopub.status.idle":"2022-03-17T23:18:25.707311Z","shell.execute_reply.started":"2022-03-17T23:18:25.649304Z","shell.execute_reply":"2022-03-17T23:18:25.706512Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# Create the RandomForest model\nrfc = RandomForestClassifier(random_state=0)\n# Fit the model\nrfc.fit(X_train, y_train)\n# Use the trained model to predict\ny_pred = rfc.predict(X_test)\n# model accuracy\nprint(f'Model train accuracy: {rfc.score(X_train, y_train)*100:.3f}%')\nprint(f'Model test accuracy: {rfc.score(X_test, y_test)*100:.3f}%')\nprint(f'Model test precision: {precision_score(y_pred,y_test):.3f}')\nprint(f'Model test recall: {recall_score(y_pred,y_test):.3f}')\nprint(f'Model test f1_score: {f1_score(y_pred,y_test):.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:19:01.899825Z","iopub.execute_input":"2022-03-17T23:19:01.900147Z","iopub.status.idle":"2022-03-17T23:19:07.418349Z","shell.execute_reply.started":"2022-03-17T23:19:01.900112Z","shell.execute_reply":"2022-03-17T23:19:07.417402Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"rfc_performance = ML_models_performance(rfc, X_train, y_train ,X_test ,y_test, y_pred, \"Random Forest\")\nrfc_performance","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:19:15.002746Z","iopub.execute_input":"2022-03-17T23:19:15.003788Z","iopub.status.idle":"2022-03-17T23:19:15.236333Z","shell.execute_reply.started":"2022-03-17T23:19:15.003712Z","shell.execute_reply":"2022-03-17T23:19:15.235541Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# Create the SVC model with some regularization\nsvcl = SVC(kernel='linear', random_state=1, C=1/2)\n# Fit the model\nsvcl.fit(X_train, y_train)\n# Use the trained model to predict\ny_pred = svcl.predict(X_test)\n# model accuracy\nprint(f'Model train accuracy: {svcl.score(X_train, y_train)*100:.3f}%')\nprint(f'Model test accuracy: {svcl.score(X_test, y_test)*100:.3f}%')\nprint(f'Model test precision: {precision_score(y_pred,y_test):.3f}')\nprint(f'Model test recall: {recall_score(y_pred,y_test):.3f}')\nprint(f'Model test f1_score: {f1_score(y_pred,y_test):.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:19:40.403747Z","iopub.execute_input":"2022-03-17T23:19:40.404058Z","iopub.status.idle":"2022-03-17T23:20:51.726521Z","shell.execute_reply.started":"2022-03-17T23:19:40.404026Z","shell.execute_reply":"2022-03-17T23:20:51.725571Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"comparison_df = pd.concat([lr_performance, LSVC_performance, rfc_performance])\ncomparison_df","metadata":{"execution":{"iopub.status.busy":"2022-03-17T23:21:04.923526Z","iopub.execute_input":"2022-03-17T23:21:04.923832Z","iopub.status.idle":"2022-03-17T23:21:04.936460Z","shell.execute_reply.started":"2022-03-17T23:21:04.923778Z","shell.execute_reply":"2022-03-17T23:21:04.935851Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"It seems using the bag of words representation of texts plus using the LinearSVC model gives the best model in terms of the accuracy and f1_score.\nIn order to improve the ML models for this classification, I guess we can play with the citation threshold for the hot and not-hot topics.\nAlso, we can play with the hyper parameter tuning of the different models (such as changing the regularization parameter), because of the overfitting. \nAlso, the number of data is limited here (only 1932). Overfitting can be decreased with increasing the data. ","metadata":{}}]}